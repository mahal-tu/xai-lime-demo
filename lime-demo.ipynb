{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:29:29.967034Z",
     "iopub.status.busy": "2023-10-03T09:29:29.966435Z",
     "iopub.status.idle": "2023-10-03T09:29:32.828839Z",
     "shell.execute_reply": "2023-10-03T09:29:32.828007Z"
    },
    "id": "TTBSvHcSLBzc"
   },
   "outputs": [],
   "source": [
    "# mini MNIST\n",
    "# sampled from original MNIST data, e.g., https://huggingface.co/datasets/ylecun/mnist\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x = pd.read_csv(\"mnist-imgs-mini.csv.bz2\", index_col=0)\n",
    "x = x/255.0\n",
    "x = x.to_numpy()\n",
    "print(x.shape, x.dtype)\n",
    "\n",
    "y = pd.read_csv(\"mnist-lbls-mini.csv.bz2\", index_col=0)\n",
    "y = y.to_numpy()\n",
    "print(y.shape, y.dtype)\n",
    "\n",
    "label_names = range(10)\n",
    "labels = list(label_names)\n",
    "print(label_names)\n",
    "\n",
    "n_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some random examples\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# based on https://www.kaggle.com/code/gainknowledge/mnist-scikit-learn-tutorial\n",
    "# Apache 2.0 License\n",
    "\n",
    "width = int(math.sqrt(x.shape[1] / n_channels))\n",
    "\n",
    "def displayData(X,Y):\n",
    "    # set up array\n",
    "    grid = 7\n",
    "    fig, ax = plt.subplots(nrows=grid, ncols=grid, figsize=(15,15))\n",
    "    # loop over randomly drawn numbers\n",
    "    for i in range(grid):\n",
    "        for j in range(grid):\n",
    "            ind = np.random.randint(X.shape[0])\n",
    "            if (n_channels == 1):\n",
    "                tmp = X[ind,:].reshape(width, width)\n",
    "                ax[i,j].imshow(tmp, cmap='gray_r') # display it as gray colors.\n",
    "            else:\n",
    "                tmp = X[ind,:].reshape(width, width, n_channels)\n",
    "                ax[i,j].imshow(tmp) # display it as rgb colors.\n",
    "            ax[i,j].set_title(label_names[Y[ind][0]])\n",
    "            ax[i,j].axis('off')\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "print(x.shape)\n",
    "displayData(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train SVM on classes\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "n_samples = 1000\n",
    "train_rows = np.random.choice(x.shape[0], size=n_samples)\n",
    "classifier_svm = SVC(probability=True) #), gamma=0.005, C=8.0)\n",
    "classifier_svm.fit(x[train_rows], y[train_rows])\n",
    "pred = classifier_svm.predict(x[train_rows])\n",
    "#ConfusionMatrixDisplay.from_estimator(true_svm, x[train_rows], y[train_rows], labels=labels, normalize=\"true\", values_format='.2f')\n",
    "ConfusionMatrixDisplay.from_predictions(y[train_rows], pred, labels=labels, normalize=\"true\", values_format='.2f')\n",
    "print(label_names)\n",
    "print(f'f1 = {f1_score(y[train_rows], pred, average=\"micro\", labels=labels)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: save svm\n",
    "import pickle\n",
    "with open(\"cls.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifier_svm, f, protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: load svm\n",
    "import pickle\n",
    "with open(\"cls.pkl\", \"rb\") as f:\n",
    "    classifier_svm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find interesting classifications\n",
    "import math\n",
    "\n",
    "n_samples = min(x.shape[0], 10000)\n",
    "selected_rows = np.random.choice(x.shape[0], size=n_samples)\n",
    "pred_proba = classifier_svm.predict_proba(x[selected_rows])\n",
    "pred_cls = classifier_svm.predict(x[selected_rows])\n",
    "ground_truth = y[selected_rows]\n",
    "\n",
    "cls_of_interest = []\n",
    "correct = 0\n",
    "true_mismatch_proba = 0\n",
    "for idx in range(n_samples):\n",
    "    pred_cls_1st = np.argmax(pred_proba[idx])\n",
    "\n",
    "    if pred_cls_1st == ground_truth[idx]:\n",
    "        correct = correct + 1\n",
    "\n",
    "    if pred_cls_1st != pred_cls[idx]:\n",
    "        # print(f\"true mismatch #{idx}: pred={pred_true[idx]} max_prob={pred_true_1st} p={pred_true_p[idx][pred_true_1st]} y={y[idx][0]}\")\n",
    "        true_mismatch_proba = true_mismatch_proba + 1\n",
    "        continue\n",
    "\n",
    "    proba_1st = pred_proba[idx][pred_cls_1st]\n",
    "    pred_proba[idx][pred_cls_1st] = 0 \n",
    "\n",
    "    pred_cls_2nd = np.argmax(pred_proba[idx])\n",
    "    proba_2nd = pred_proba[idx][pred_cls_2nd]\n",
    "\n",
    "    # ignore if ground truth is not in first two guesses\n",
    "    if (pred_cls_2nd != ground_truth[idx]) & (pred_cls_1st != ground_truth[idx]):\n",
    "        continue\n",
    "\n",
    "    # ignore if classifier is super shure\n",
    "    if proba_1st - proba_2nd > .5: continue\n",
    "\n",
    "    if pred_cls_1st != ground_truth[idx]:\n",
    "        cls_of_interest.append([selected_rows[idx], pred_cls_1st, pred_cls_2nd, ground_truth[idx][0], [proba_1st, proba_2nd]])\n",
    "        \n",
    "\n",
    "print(f\"correct: {correct/n_samples}\")\n",
    "print(f\"mismatch between pred and probs: true={true_mismatch_proba/n_samples}\")\n",
    "print(f\"interesting examples {len(cls_of_interest) / n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain with lime\n",
    "\n",
    "# Ribeiro, M. T., Singh, S. & Guestrin, C. (2016). Why should I trust you? Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (S. 1135â€“1144). doi: 10.1145/2939672.2939778\n",
    "# cmp \"official\" tutorial at https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20MNIST%20and%20RF.ipynb\n",
    "# BSD 2-clause\n",
    "\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "from skimage import color as sk_color\n",
    "from skimage.color import label2rgb\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "n_pixels = x.shape[1]\n",
    "width = int(math.sqrt(n_pixels / n_channels))\n",
    "\n",
    "setup = [[1, classifier_svm]]\n",
    "\n",
    "explainer = LimeImageExplainer()\n",
    "# segmenter -> return int matrix of same shape with superpixel index in each cell\n",
    "\n",
    "def split_evenly(img, n_splits):\n",
    "    grid = np.zeros((img.shape[0], img.shape[1]), dtype=int)\n",
    "    grid_width_x = img.shape[0] / n_splits\n",
    "    grid_width_y = img.shape[1] / n_splits\n",
    "    for col in range(img.shape[0]):\n",
    "        x_grid = math.trunc(col / grid_width_x)\n",
    "        for row in range(img.shape[1]):\n",
    "            y_grid = math.trunc(row / grid_width_y)\n",
    "            grid[col][row] = x_grid + y_grid * n_splits\n",
    "    return grid\n",
    "\n",
    "\n",
    "grids = {}\n",
    "def grid_n_parts(img, splits):\n",
    "    grid_idx = (img.shape[0], img.shape[1], splits)\n",
    "    if grid_idx in grids:\n",
    "        return grids[grid_idx]\n",
    "    else:\n",
    "        grid = split_evenly(img, splits)\n",
    "        grids[grid_idx] = grid\n",
    "        return grid\n",
    "\n",
    "def grid_5parts(img):\n",
    "    return grid_n_parts(img, 5)\n",
    "\n",
    "def grid_7parts(img):\n",
    "    return grid_n_parts(img, 7)\n",
    "\n",
    "def grid_11parts(img):\n",
    "    return grid_n_parts(img, 11)\n",
    "\n",
    "# 'quickshift', 'slic', 'felzenszwalb'\n",
    "# https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.quickshift\n",
    "# ratio: color=1 space=0\n",
    "quickshift = SegmentationAlgorithm('quickshift')\n",
    "# https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.felzenszwalb\n",
    "felzenszwalb = SegmentationAlgorithm('felzenszwalb', sigma=width/20, scale=width/50, channel_axis=2)\n",
    "# https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.slic\n",
    "slic_sma = SegmentationAlgorithm('slic', n_segments=25, compactness=.01, convert2lab=False, enforce_connectivity=True)\n",
    "slic_mid = SegmentationAlgorithm('slic', n_segments=25, compactness=0.1, convert2lab=False, enforce_connectivity=True)\n",
    "slic_lrg = SegmentationAlgorithm('slic', n_segments=25, compactness=1.0, convert2lab=False, enforce_connectivity=True)\n",
    "\n",
    "\n",
    "## in case different aggregation functions should be used for some of the channels (e.g. value)\n",
    "def average_hsvs(imgs):\n",
    "    c = np.zeros_like(imgs[0])\n",
    "    for img in imgs:\n",
    "        c += img\n",
    "    c /= len(imgs)\n",
    "    return c\n",
    "\n",
    "## combine several hsv images, with hue being weighted by the saturation\n",
    "def combine_hsvs(imgs):\n",
    "    c = np.zeros_like(imgs[0])\n",
    "    for col in range(c.shape[0]):\n",
    "        for row in range(c.shape[1]):\n",
    "            huevec = [0, 0]\n",
    "            saturation = 0\n",
    "            value = 0\n",
    "            for img in imgs:\n",
    "                huevec[0] += math.cos(img[col][row][0]*math.pi*2) * img[col][row][1]\n",
    "                huevec[1] += math.sin(img[col][row][0]*math.pi*2) * img[col][row][1]\n",
    "                saturation += img[col][row][1]\n",
    "                value += img[col][row][2]\n",
    "\n",
    "            c[col][row][0] = math.atan2(huevec[1], huevec[0])/math.pi/2\n",
    "            c[col][row][1] = saturation / len(imgs)\n",
    "            c[col][row][2] = value / len(imgs)\n",
    "    return c\n",
    "\n",
    "def combine_weights(ws):\n",
    "    c = np.zeros_like(ws[0])\n",
    "    for w in ws:\n",
    "        c += w\n",
    "    norm = max(abs(np.min(c)), np.max(c))\n",
    "    c /= norm\n",
    "    return c\n",
    "\n",
    "white = np.ones((width,width))\n",
    "HUE_RED = 0.0 # 0 deg\n",
    "HUE_GREEN = 1.0/3.0 # 120 deg\n",
    "VALUE_ADD = 0.5 # how much should the computed weight also increases the value\n",
    "\n",
    "N_SAMPLES = 4\n",
    "\n",
    "print(f\"{len(cls_of_interest)} examples\")\n",
    "\n",
    "for trial in random.sample(cls_of_interest, N_SAMPLES):\n",
    "    idx = trial[0]\n",
    "    pred_1st = trial[1]\n",
    "    pred_2nd = trial[2]\n",
    "    true_class = trial[3]\n",
    "\n",
    "    print(f'{idx}: 1st {pred_1st}, 2nd {pred_2nd}, true {true_class}, c_1 {trial[4]}')\n",
    "\n",
    "    for s in setup:\n",
    "        clsf_id = s[0]\n",
    "        classifier = s[1]\n",
    "\n",
    "        def predict_instance(img_batch):\n",
    "            if n_channels == 1:\n",
    "                # back to gray scale and adjust dimensionality\n",
    "                gray = sk_color.rgb2gray(img_batch).reshape((-1, width*width))\n",
    "                pred = classifier.predict_proba(gray)\n",
    "            else:\n",
    "                # adjust dimensionality\n",
    "                pred = classifier.predict_proba(img_batch.reshape((-1, width*width*n_channels)))\n",
    "\n",
    "            return pred\n",
    "\n",
    "        if n_channels == 1:\n",
    "            img_data = x[idx].reshape((width, width))\n",
    "            img = sk_color.gray2rgb(img_data)\n",
    "            # MNIST is inverted\n",
    "            orig_img = sk_color.gray2rgb(white - img_data)\n",
    "        else:\n",
    "            img_data = x[idx].reshape((width, width, n_channels))\n",
    "            # no need to invert\n",
    "            img = img_data\n",
    "            orig_img = img\n",
    "\n",
    "        def get_expl_weights(segm_fn):\n",
    "            explanation = explainer.explain_instance(img, predict_instance, \n",
    "                                                    num_features=100,\n",
    "                                                    labels=(pred_1st, pred_2nd),\n",
    "                                                    hide_color=None, num_samples=500,\n",
    "                                                    segmentation_fn=segm_fn,\n",
    "                                                    batch_size=100)\n",
    "\n",
    "            p1st_local_weights = explanation.local_exp[pred_1st] # list of (feat-id, weight)\n",
    "            p2nd_local_weights = explanation.local_exp[pred_2nd]\n",
    "\n",
    "            p1st_max_weight = abs(p1st_local_weights[0][1])\n",
    "            p2nd_max_weight = abs(p2nd_local_weights[0][1])\n",
    "\n",
    "            p1weights = {}\n",
    "            for w in p1st_local_weights:\n",
    "                p1weights[w[0]] = w[1]\n",
    "            p2weights = {}\n",
    "            for w in p2nd_local_weights:\n",
    "                p2weights[w[0]] = w[1]\n",
    "\n",
    "            super_pixels = explanation.segments\n",
    "            w1 = np.zeros_like(super_pixels, dtype=np.float32)\n",
    "            w2 = np.zeros_like(super_pixels, dtype=np.float32)\n",
    "\n",
    "            for col in range(img.shape[0]):\n",
    "                for row in range(img.shape[1]):\n",
    "                    pix_id = super_pixels[col][row]\n",
    "                    if pix_id in p1weights:\n",
    "                        w1[col][row] = p1weights[pix_id] / p1st_max_weight\n",
    "                    if pix_id in p2weights:\n",
    "                        w2[col][row] = p2weights[pix_id] / p2nd_max_weight\n",
    "\n",
    "            return w1, w2\n",
    "        \n",
    "        def weights2mask(w):\n",
    "            base_color = np.mean(orig_img)\n",
    "            bg_color = np.array([base_color,base_color,base_color])\n",
    "            white = np.array([1,1,1])\n",
    "\n",
    "            result_img = np.copy(orig_img) # todo create empty image in img.size and init with bgcolor?\n",
    "            for col in range(result_img.shape[0]):\n",
    "                for row in range(result_img.shape[1]):\n",
    "                    if w[col][row]>0:\n",
    "                        weight = math.pow(w[col][row], 1.0)\n",
    "                        result_img[col][row] = weight * orig_img[col][row] + (1-weight) * white\n",
    "                    else:\n",
    "                        result_img[col][row] = white\n",
    "            return result_img\n",
    "\n",
    "        def weights2hsv(w):\n",
    "            result_img = sk_color.rgb2hsv(orig_img)\n",
    "            for col in range(w.shape[0]):\n",
    "                for row in range(w.shape[1]):\n",
    "                    #weight = weight_trans(abs(w[col][row]))\n",
    "                    weight = math.pow(abs(w[col][row]), 1.0)\n",
    "                    # saturation\n",
    "                    result_img[col][row][1] = weight\n",
    "                    # increase value (to get visible colors in dark areas)\n",
    "                    result_img[col][row][2] = min(1, weight*VALUE_ADD + result_img[col][row][2])\n",
    "                    # TODO blend between weight color and original color\n",
    "                    if w[col][row] > 0:\n",
    "                        result_img[col][row][0] = HUE_GREEN # positive weight -> green\n",
    "                    elif w[col][row] < 0: \n",
    "                        result_img[col][row][0] = HUE_RED # negative weight -> red\n",
    "            return result_img\n",
    "        \n",
    "        def make_plot(wimg1, wimg2, with_p, seg_id):\n",
    "            inverse_order = False\n",
    "            p1 = math.trunc(100 * trial[3+clsf_id][0] + .5)\n",
    "            p2 = math.trunc(100 * trial[3+clsf_id][1] + .5)\n",
    "\n",
    "            fig, ((orig, img_1st, img_2nd)) = plt.subplots(1, 3, figsize=(4,2))\n",
    "            orig.set_title('Original')\n",
    "            orig.imshow(orig_img)\n",
    "            orig.axis('off')\n",
    "\n",
    "            if with_p:\n",
    "                img_1st.set_title(f'{p1}% {label_names[pred_2nd if inverse_order else pred_1st]}')\n",
    "            else:\n",
    "                img_1st.set_title(f'A: {label_names[pred_2nd if inverse_order else pred_1st]}')\n",
    "            img_1st.imshow(wimg2 if inverse_order else wimg1)\n",
    "            img_1st.axis('off')\n",
    "\n",
    "            if with_p:\n",
    "                img_2nd.set_title(f'{p2}% {label_names[pred_1st if inverse_order else pred_2nd]}')\n",
    "            else:\n",
    "                img_2nd.set_title(f'B: {label_names[pred_1st if inverse_order else pred_2nd]}')\n",
    "            img_2nd.imshow(wimg1 if inverse_order else wimg2)\n",
    "            img_2nd.axis('off')\n",
    "            p_on = 1 if with_p else 0\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"lime-imgs/expl-i{idx}-t{true_class}-c{clsf_id}-{seg_id}-p{p_on}.png\")\n",
    "            #plt.close()\n",
    "\n",
    "        try:\n",
    "            (w1a, w2a) = get_expl_weights(slic_sma)\n",
    "            (w1b, w2b) = get_expl_weights(slic_mid)\n",
    "            (w1c, w2c) = get_expl_weights(slic_lrg)\n",
    "            (w1d, w2d) = get_expl_weights(grid_5parts)\n",
    "            (w1e, w2e) = get_expl_weights(grid_7parts)\n",
    "            (w1f, w2f) = get_expl_weights(grid_11parts)\n",
    "            #(w1g, w2g) = get_expl_weights(felzenszwalb)\n",
    "            #(w1h, w2h) = get_expl_weights(quickshift)\n",
    "            w1ac = combine_weights([w1a, w1b, w1c])\n",
    "            w2ac = combine_weights([w2a, w2b, w2c])\n",
    "            w1df = combine_weights([w1d, w1e, w1f])\n",
    "            w2df = combine_weights([w2d, w2e, w2f])\n",
    "            w1af = combine_weights([w1a, w1b, w1c, w1d, w1e, w1f])\n",
    "            w2af = combine_weights([w2a, w2b, w2c, w2d, w2e, w2f])\n",
    "\n",
    "            # HSV version\n",
    "            make_plot(sk_color.hsv2rgb(weights2hsv(w1af)), \n",
    "                    sk_color.hsv2rgb(weights2hsv(w2af)), True, \"haf\")\n",
    "\n",
    "            # Mask parts of the image\n",
    "            make_plot(weights2mask(w1af), \n",
    "                      weights2mask(w2af), True, \"maf\")\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"could not create explanation for trial {idx}, classifier {clsf_id}\", err)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow/datasets",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
